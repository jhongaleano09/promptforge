# 05. Loops de Refinamiento y System Prompts

**Objetivo:** Cerrar el ciclo de producciÃ³n. Transformar la herramienta de un generador lineal a un sistema de **mejora continua**. Implementar la capacidad de probar los prompts con inputs reales (Testing), evaluar sus respuestas mediante el mismo modelo (Juez LLM) y refinarlos iterativamente.

## ðŸ§  Estrategia TÃ©cnica

Esta fase introduce la **circularidad** en el flujo de trabajo:
1.  **Testing Real (LiteLLM):** EjecuciÃ³n de variantes contra modelos reales (APIs compatibles con OpenAI).
2.  **Juez AutomÃ¡tico (Self-Correction):** El mismo modelo configurado actÃºa como crÃ­tico de sus propias variantes, recomendando la mejor opciÃ³n.
3.  **Memoria CÃ­clica:** LangGraph mantiene un historial de iteraciones para permitir refinamiento y "undo".

---

## ðŸ› ï¸ Tareas TÃ©cnicas Detalladas

### 5.1 Motor de EjecuciÃ³n (Testing Engine)
Implementar la capa que permite "disparar" los prompts generados.

- [ ] **Servicio de EjecuciÃ³n (Backend):**
    - FunciÃ³n `run_prompt_variant(variant_id, input_data, llm_config)` usando `LiteLLM`.
    - **Soporte de APIs:** Priorizar OpenAI, OpenRouter, DeepSeek, Z.AI, MiniMax. (Ollama queda preparado arquitectÃ³nicamente pero no es la prioridad inmediata).
    - **InyecciÃ³n de Variables:** Detectar variables (ej: `{{ input_usuario }}`) y solicitar valores antes de ejecutar.
- [ ] **Control de Costos (UI):**
    - **Advertencia:** Mostrar notificaciÃ³n/modal antes de ejecutar: *"Esta acciÃ³n ejecutarÃ¡ X llamadas al modelo. Consumo estimado de tokens."*
    - ConfirmaciÃ³n explÃ­cita del usuario requerida.

### 5.2 LÃ³gica Especial: System Prompts
Tratamiento diferenciado cuando `prompt_type == 'system'`.

- [ ] **UI de Input de Prueba:**
    - Agregar campo de texto global "User Input (Prueba)" en la Arena.
    - Este input se aplica simultÃ¡neamente a las 3 variantes.
- [ ] **ConstrucciÃ³n del Payload:**
    - Payload: `messages = [{role: "system", content: variante_X}, {role: "user", content: input_prueba}]`.
- [ ] **VisualizaciÃ³n Paralela:**
    - Mostrar las 3 respuestas del modelo (Output A, B, C) en columnas adyacentes para comparaciÃ³n directa.

### 5.3 El Agente Juez (Auto-Eval)
Un agente que analiza los outputs y ofrece un veredicto.

- [ ] **ConfiguraciÃ³n del Juez:**
    - Utilizar el **mismo modelo y API Key** configurados por el usuario (sin coste extra de modelos externos).
- [ ] **Prompt del Juez:**
    - *Rol:* CrÃ­tico experto.
    - *Input:* IntenciÃ³n original + Test Input + 3 Outputs generados.
    - *Output:* JSON con `{ winner: "A", reason: "MÃ¡s concisa y segura", highlights: ["Creativo", "Formal"] }`.
- [ ] **IntegraciÃ³n en Arena:**
    - Badge "Recomendado por la IA" sobre la variante ganadora.
    - Notas cortas: "Destaca en: X, Y".

### 5.4 Ciclo de Refinamiento (LangGraph Loop)
Modificar el grafo para permitir volver atrÃ¡s y mejorar.

- [ ] **ActualizaciÃ³n de `PromptState`:**
    - Agregar campo `history: List[PromptStateSnapshot]`.
    - **LÃ­mite:** Mantener las Ãºltimas **100 iteraciones** (FIFO). Las mÃ¡s antiguas se eliminan.
- [ ] **Nodo Refinador:**
    - *Input:* Variante seleccionada + Feedback del usuario (ej: "Hazlo mÃ¡s corto").
    - *AcciÃ³n:* Generar 3 nuevas versiones (V2.A, V2.B, V2.C) basadas en la ganadora y el feedback.
    - *CombinaciÃ³n:* Permitir al usuario seleccionar partes de A y B (manual) o pedir al modelo que las fusione.

---

## ðŸ’¾ Modelo de Datos (Actualizaciones)

Se expande el esquema de `PromptState` para soportar ejecuciones e historia.

```python
class PromptState(TypedDict):
    # ... campos existentes ...
    iteration: int
    history: List[dict] # Snapshots de estados anteriores (Max 100)
    
    # Resultados del Testing
    test_inputs: dict # { 'variable_1': 'valor' }
    test_outputs: dict # { 'A': 'Respuesta modelo...', 'B': ... }
    
    # EvaluaciÃ³n
    judge_result: dict # { 'winner': 'A', 'reason': '...', 'tags': [...] }
```

---

## ðŸ”Œ API Endpoints Nuevos

- `POST /api/arena/execute`: Recibe `variant_ids` y `test_input`. Retorna outputs de LLM.
- `POST /api/arena/judge`: Ejecuta el Agente Juez sobre los outputs actuales.
- `POST /api/workflow/{thread_id}/refine`: EnvÃ­a feedback y selecciÃ³n para iniciar nueva iteraciÃ³n.
- `POST /api/workflow/{thread_id}/history`: Obtiene la lista de versiones anteriores.
- `POST /api/workflow/{thread_id}/rollback`: Restaura un estado anterior.

---

## âœ… Criterios de AceptaciÃ³n (DoD)

1.  **Testing Funcional:** El usuario puede ingresar un input, confirmar la advertencia de costos, y ver las 3 respuestas del LLM.
2.  **Juez Inteligente:** El sistema marca automÃ¡ticamente la mejor respuesta usando el mismo modelo configurado.
3.  **Refinamiento Efectivo:** El usuario puede pedir cambios ("MÃ¡s formal") y obtener versiones nuevas coherentes.
4.  **GestiÃ³n de Memoria:** El historial permite "undo" y respeta el lÃ­mite de 100 iteraciones.
5.  **Multi-API:** Funciona correctamente con proveedores compatibles con OpenAI (OpenRouter, DeepSeek, etc.).

---

> **Decisiones de DiseÃ±o:**
> 1. **Juez:** Usa el mismo modelo del usuario (no un modelo externo fijo).
> 2. **APIs:** Prioridad a APIs estÃ¡ndar sobre ejecuciÃ³n local (Ollama) en esta fase.
> 3. **LÃ­mite:** Historial rotativo de 100 pasos.
