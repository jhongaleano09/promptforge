# 09. Fase: Validaci√≥n de API Key de Test

**Estado:** üÜï PLANIFICADA - Lista para Implementaci√≥n  
**Prioridad:** 4 (BAJA - Solo para desarrollador/propietario)  
**Estimado:** 1-2 d√≠as

---

## üéØ Objetivos

Implementar un sistema de validaci√≥n de API key exclusiva para pruebas que:
1. Solo el propietario (desarrollador) pueda usar la API key de test
2. La API key de test NO se guarde en la base de datos
3. La API key de test NO aparezca en la UI de usuarios normales
4. Validaci√≥n temporal sin persistencia (solo para pruebas)
5. Seguridad para evitar uso no autorizado

---

## üó∫ Desglose de Tareas

### Tarea 9.1: Crear Endpoint de Validaci√≥n Especial

**Archivo:** `backend/app/api/endpoints.py`

**Objetivo:** Implementar endpoint `/api/settings/validate-test` que valide API key sin guardarla en base de datos.

**Diferencias con `/api/settings/validate`:**

| Aspecto | `/api/settings/validate` | `/api/settings/validate-test` |
|---------|-------------------------------|--------------------------------|
| **Guarda en BD** | ‚úÖ S√≠ | ‚ùå NO |
| **Aparece en UI normal** | ‚úÖ S√≠ | ‚ùå NO |
| **Uso** | Producci√≥n (usuarios finales) | Solo pruebas del propietario |
| **Persistencia** | Permanente | Temporal (sin guardar) |
| **Accesibilidad** | P√∫blica (requiere autenticaci√≥n) | Restringida (modo especial) |

**Implementaci√≥n:**

#### 9.1.1: Estructura del Endpoint

```python
@router.post("/settings/validate-test")
async def validate_test_key(request: ValidationRequest):
    """
    Valida una API key SIN guardarla en base de datos.
    Solo para pruebas del propietario/desarrollador.
    
    Endpoint diferente de /settings/validate que:
    - Valida Y guarda en BD
    - Es para producci√≥n (usuarios finales)
    
    Args:
        request: ValidationRequest con provider, api_key
    
    Returns:
        JSON con resultado de validaci√≥n
        
    Raises:
        HTTPException: Si la validaci√≥n falla
    """
    # Implementaci√≥n detallada en tareas siguientes
    pass
```

#### 9.1.2: Validaci√≥n con Servicio LLM

**Objetivo:** Llamar realmente al servicio (OpenAI, Anthropic, etc.) para verificar que la API key funciona.

**Pasos de Implementaci√≥n:**

1. **Obtener modelo de prueba seg√∫n proveedor**
   - OpenAI: `gpt-3.5-turbo` (econ√≥mico)
   - Anthropic: `claude-3-haiku-20240307` (econ√≥mico)
   - Ollama: `llama3` (local, no tiene costo)

2. **Construir mensaje de prueba**
   - Simple: "Hello" o "Test"
   - Corto: m√°ximo 5-10 tokens
   - Objetivo: validar r√°pidamente sin gastar mucho

3. **Llamar al servicio con LiteLLM**
```python
# Pseudoc√≥digo
try:
    response = await litellm.acompletion(
        model=test_model,
        messages=[{"role": "user", "content": test_message}],
        api_key=request.api_key,
        max_tokens=5
    )
    
    # Validaci√≥n exitosa
    return {
        "status": "success",
        "message": "API Key is valid",
        "provider": request.provider,
        "test_response": response.choices[0].message.content
    }
    
except AuthenticationError as e:
    # API key inv√°lida
    raise HTTPException(
        status_code=401,
        detail="Invalid API Key. Please check your credentials."
    )
    
except RateLimitError as e:
    # L√≠mite de cuota o rate limit
    raise HTTPException(
        status_code=429,
        detail="Rate limit exceeded or insufficient quota. Please check your OpenAI dashboard."
    )
    
except Exception as e:
    # Error general
    raise HTTPException(
        status_code=500,
        detail=f"Validation failed: {str(e)}"
    )
```

#### 9.1.3: Validaci√≥n del Proveedor

**Objetivo:** Asegurar que el proveedor sea v√°lido antes de intentar validar.

**Validaciones:**
```python
# Lista de proveedores soportados
SUPPORTED_PROVIDERS = ["openai", "anthropic", "ollama"]

# Validar que el proveedor est√© en la lista
if request.provider not in SUPPORTED_PROVIDERS:
    raise HTTPException(
        status_code=400,
        detail=f"Provider '{request.provider}' is not supported. Supported providers: {', '.join(SUPPORTED_PROVIDERS)}"
    )
```

**Consideraciones:**
- Case-insensitive (aceptar "OpenAI", "OPENAI", "openai")
- Mensaje de error claro y espec√≠fico
- Listar proveedores soportados en el mensaje

#### 9.1.4: Rate Limiting (Opcional pero Recomendado)

**Objetivo:** Prevenir abusos del endpoint de validaci√≥n de prueba.

**Implementaci√≥n:**
```python
from slowapi import Request
from functools import wraps
import time
from collections import defaultdict

# Almacenamiento en memoria (simple para prototipo)
validation_attempts = defaultdict(list)
RATE_LIMIT = 10  # M√°ximo 10 validaciones por hora
RATE_WINDOW = 3600  # 1 hora en segundos

def rate_limit_decorator(request: Request):
    """Decorator para rate limiting en validaciones de prueba."""
    client_ip = request.client.host
    current_time = time.time()
    
    # Limpiar intentos antiguos
    validation_attempts[client_ip] = [
        attempt for attempt in validation_attempts[client_ip]
        if current_time - attempt < RATE_WINDOW
    ]
    
    # Verificar l√≠mite
    if len(validation_attempts[client_ip]) >= RATE_LIMIT:
        raise HTTPException(
            status_code=429,
            detail=f"Rate limit exceeded. Maximum {RATE_LIMIT} validations per hour."
        )
    
    # Guardar intento
    validation_attempts[client_ip].append(current_time)
    return True

# Aplicar al endpoint
@router.post("/settings/validate-test")
@rate_limit_decorator  # Aplicar rate limiting
async def validate_test_key(request: ValidationRequest):
    # ... l√≥gica de validaci√≥n
    pass
```

**Mejoras futuras (no implementar ahora):**
- Usar Redis para rate limiting distribuido
- Implementar rate limiting por API key (no solo por IP)
- Agregar cooldown entre validaciones fallidas

**‚ùì Preguntas Clave:**

1. ¬øDeseas implementar rate limiting ahora (recomendado) o dejarlo para una fase posterior?
2. ¬øDeber√≠amos usar 10 validaciones por hora o un n√∫mero diferente?
3. ¬øDeseas implementar el rate limiting con un decorador de Python o con middleware de FastAPI?
4. ¬øQu√© deber√≠a pasar si se excede el l√≠mite? ¬øError HTTP 429 o permitir con un warning?

#### 9.1.5: Logging de Validaciones de Prueba

**Objetivo:** Registrar todas las validaciones de API key de prueba para auditor√≠a.

**Implementaci√≥n:**
```python
import logging

# Configurar logger espec√≠fico para validaciones de prueba
test_validation_logger = logging.getLogger("test_validations")
test_validation_logger.setLevel(logging.INFO)

# Handler personalizado
handler = logging.FileHandler('logs/test_validations.log')
handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
test_validation_logger.addHandler(handler)

@router.post("/settings/validate-test")
async def validate_test_key(request: Request, validation_request: ValidationRequest):
    # ... l√≥gica de validaci√≥n
    
    # Registrar la validaci√≥n
    client_ip = request.client.host
    result_status = "success" if "status" in response else "error"
    
    test_validation_logger.info(
        f"IP: {client_ip} | Provider: {request.provider} | "
        f"Status: {result_status} | Message: {response.get('message', 'N/A')}"
    )
    
    return response
```

**Informaci√≥n registrada:**
- IP del cliente
- Proveedor
- Status (success/error)
- Mensaje de resultado o error
- Timestamp (agregado autom√°ticamente por el logger)

**Consideraciones:**
- NO guardar la API key en el log (seguridad)
- Guardar solo metadatos (IP, proveedor, status)
- Rotar logs peri√≥dicamente (no crecer indefinidamente)

**‚ùì Preguntas Clave:**

1. ¬øDeseas guardar el log en `logs/test_validations.log` o usar la ruta `backend.log` existente?
2. ¬øDeseas agregar el user agent del cliente en el log o solo la IP?
3. ¬øDeber√≠amos usar diferentes niveles de logging (INFO, WARNING, ERROR) seg√∫n el resultado de la validaci√≥n?
4. ¬øDeber√≠amos agregar tambi√©n el timestamp exacto en formato ISO 8601 en el mensaje del log?
5. ¬øC√≥mo deseas que se manejen los logs en producci√≥n? ¬øRotar autom√°ticamente o archivar por fecha?

---

### Tarea 9.2: Modo de Test para Propietario

**Opci√≥n A: Variable de Entorno (RECOMENDADA)**

**Objetivo:** Permitir habilitar un "modo de test" mediante variable de entorno.

**Implementaci√≥n en backend:**

1. **Agregar validaci√≥n en endpoints existentes**
```python
import os

@router.post("/settings/validate-test")
async def validate_test_key(request: ValidationRequest):
    # Verificar si estamos en modo de test
    test_mode = os.getenv("PROMPTFORGE_TEST_MODE", "false").lower() == "true"
    
    if not test_mode:
        raise HTTPException(
            status_code=403,
            detail="Test validation endpoint is only available in test mode. Set PROMPTFORGE_TEST_MODE=true to enable."
        )
    
    # ... l√≥gica de validaci√≥n
    pass
```

2. **Documentaci√≥n de variables de entorno**
- Agregar a `.env.example`:
```bash
# Modo de test para validaci√≥n de API keys sin persistencia
# WARNING: Solo habilitar si eres el propietario/desarrollador
PROMPTFORGE_TEST_MODE=false
```

**Ventajas:**
- Simple de implementar
- F√°cil de deshabilitar en producci√≥n
- No requiere cambios en el frontend

**‚ùì Preguntas Clave:**

1. ¬øDeseas que el mensaje de error sea espec√≠fico sobre que este endpoint es solo para desarrolladores o gen√©rico?
2. ¬øDeber√≠amos agregar una lista blanca de IPs que pueden usar el modo de test (solo tu IP, etc.)?
3. ¬øDeseas que el modo de test tambi√©n habilite otros endpoints de debugging o solo el de validaci√≥n?

**Opci√≥n B: Par√°metro de URL (Alternativa)**

**Objetivo:** Permitir acceder al modo de test mediante par√°metros en la URL.

**Implementaci√≥n en backend:**
```python
@router.post("/settings/validate-test")
async def validate_test_key(request: Request, validation_request: ValidationRequest):
    # Verificar si hay par√°metro de test en la query string
    test_mode = request.query_params.get("test_mode", "false").lower() == "true"
    test_key = request.query_params.get("test_key")
    
    if not test_mode or not test_key:
        raise HTTPException(
            status_code=403,
            detail="Test mode requires ?test_mode=true&test_key=<your_test_key>"
        )
    
    # Opcional: Validar que la test_key sea correcta
    # Esto agrega una capa extra de seguridad
    # if test_key != os.getenv("PROMPTFORGE_TEST_KEY"):
    #     raise HTTPException(status_code=403, detail="Invalid test key")
    
    # ... l√≥gica de validaci√≥n
    pass
```

**Uso:**
```
curl -X POST http://localhost:8001/api/settings/validate-test?test_mode=true&test_key=sk-proj... \
  -H "Content-Type: application/json" \
  -d '{"provider":"openai","api_key":"sk-proj..."}'
```

**Ventajas:**
- M√°s seguro (puedes rotar la test_key)
- Flexible (puedes habilitar/deshabilitar sin reiniciar backend)
- No requiere cambios en archivos de configuraci√≥n

**Desventajas:**
- M√°s complejo de usar (URL larga)
- Visible en la barra de direcciones (puede ser copiada)

**‚ùì Preguntas Clave:**

1. ¬øDeseas implementar Opci√≥n A (variable de entorno), Opci√≥n B (par√°metro de URL), o ambas?
2. ¬øSi implementamos ambas, cu√°l deber√≠a tener prioridad (variable de entorno vs par√°metro)?
3. ¬øDeseas que la validaci√≥n de la test_key en Opci√≥n B sea opcional o requerida?

**Opci√≥n C: Token de Validaci√≥n de Un Solo Uso (M√ÅS SEGURO)**

**Objetivo:** Generar tokens temporales que expiran despu√©s de un tiempo limitado y solo pueden usarse una vez.

**Implementaci√≥n en backend:**
```python
from app.core.security import generate_temp_token, validate_temp_token
from datetime import datetime, timedelta

@router.post("/settings/validate-test")
async def validate_test_key(request: Request, validation_request: ValidationRequest):
    # Verificar si hay un token de test v√°lido en la solicitud
    provided_token = request.query_params.get("test_token")
    
    if provided_token:
        # Validar que el token sea v√°lido y no expirado
        if not validate_temp_token(provided_token):
            raise HTTPException(
                status_code=403,
                detail="Invalid or expired test token"
            )
        # Token v√°lido, continuar
    else:
        # No hay token, generar uno
        test_token = generate_temp_token(expiry_minutes=60)  # Expira en 1 hora
        
        # Retornar el token en la respuesta
        return {
            "status": "token_generated",
            "message": "Test token generated. Use this token for validations in the next 60 minutes.",
            "test_token": test_token,
            "expires_at": (datetime.now() + timedelta(hours=1)).isoformat()
        }
    
    # ... l√≥gica de validaci√≥n de la API key
    pass
```

**Implementaci√≥n de tokens:**
```python
# backend/app/core/test_token_manager.py

from cryptography.fernet import Fernet
from datetime import datetime, timedelta
import os

# Usar la misma clave de encriptaci√≥n
fernet = Fernet(os.getenv("PROMPTFORGE_SECRET_KEY"))

# Almacenamiento en memoria (simple para prototipo)
# En producci√≥n, usar Redis o base de datos
active_tokens = {}

def generate_temp_token(expiry_minutes: int = 60) -> str:
    """
    Genera un token temporal de validaci√≥n.
    
    Args:
        expiry_minutes: Minutos hasta que el token expire
    
    Returns:
        Token encriptado
    """
    # Crear payload con timestamp de expiraci√≥n
    payload = {
        "type": "test_validation",
        "expires_at": (datetime.now() + timedelta(minutes=expiry_minutes)).isoformat(),
        "created_at": datetime.now().isoformat()
    }
    
    # Encriptar el payload
    token = fernet.encrypt(str(payload).encode()).decode()
    
    # Limpiar tokens expirados
    cleanup_expired_tokens()
    
    return token

def validate_temp_token(token: str) -> bool:
    """
    Valida si un token de prueba es v√°lido y no ha expirado.
    
    Args:
        token: Token a validar
    
    Returns:
        True si es v√°lido, False si es inv√°lido o expirado
    """
    try:
        # Desencriptar el token
        decrypted = fernet.decrypt(token.encode()).decode()
        payload = eval(decrypted)  # Parsear el JSON
        
        # Verificar tipo
        if payload.get("type") != "test_validation":
            return False
        
        # Verificar expiraci√≥n
        expires_at = datetime.fromisoformat(payload["expires_at"])
        if datetime.now() > expires_at:
            return False
        
        return True
    
    except Exception:
        return False

def cleanup_expired_tokens():
    """
    Elimina tokens expirados del almacenamiento en memoria.
    """
    current_time = datetime.now()
    
    for token_str, payload_str in list(active_tokens.items()):
        try:
            payload = eval(payload_str)
            expires_at = datetime.fromisoformat(payload["expires_at"])
            
            if current_time > expires_at:
                del active_tokens[token_str]
        except:
            del active_tokens[token_str]
```

**Ventajas:**
- M√°s seguro (tokens expiran)
- No expone la API key de test
- Puede rastrear qui√©n est√° usando el token
- F√°cil de revocar (limpiar tokens)

**Desventajas:**
- M√°s complejo de implementar
- Requiere gesti√≥n de tokens
- El usuario debe copiar el token en cada validaci√≥n

**‚ùì Preguntas Clave:**

1. ¬øDeseas implementar Opci√≥n C (tokens) o prefieres Opci√≥n A o B?
2. ¬øDeseas que los tokens expiren en 1 hora o prefieres un tiempo diferente?
3. ¬øDeber√≠amos guardar un registro de tokens generados con qu√© IP los us√≥ (para auditor√≠a)?
4. ¬øDeseas agregar un endpoint para revocar tokens manualmente?

---

### Tarea 9.3: Implementaci√≥n en Frontend - Modo de Test

**Opci√≥n A: Variable de Entorno (Backend)**

Si se usa la opci√≥n de variable de entorno (`PROMPTFORGE_TEST_MODE=true`):

**No se requiere cambios en el frontend.**

El endpoint `/api/settings/validate-test` solo estar√° disponible cuando la variable de entorno est√© activada. El frontend normal no podr√° acceder a este endpoint sin la variable activada.

**Uso para propietario/desarrollador:**
1. Configurar variable en `.env.local` o `.env` del backend:
```bash
PROMPTFORGE_TEST_MODE=true
```

2. Reiniciar backend
3. Usar `curl` o Postman para probar el endpoint:
```bash
curl -X POST http://localhost:8001/api/settings/validate-test \
  -H "Content-Type: application/json" \
  -d '{"provider":"openai","api_key":"sk-proj-wzf0..."}'
```

4. Verificar que la API key de test NO se guarde en la base de datos

**Opci√≥n B: Par√°metro de URL (Backend + Frontend)**

Si se usa la opci√≥n de par√°metro de URL (`?test_mode=true&test_key=sk-...`):

**Implementaci√≥n en frontend:**

1. **Crear componente de "Modo de Test"** (opcional)
```typescript
'use client';

import { useState } from 'react';

export function TestModePanel() {
  const [testMode, setTestMode] = useState(false);
  const [testKey, setTestKey] = useState('');

  return (
    <div className="p-4 border rounded-lg bg-orange-50">
      <h3 className="font-semibold mb-2">üß™ Modo de Test</h3>
      
      <div className="space-y-3">
        <div>
          <label className="text-sm font-medium">Activar Modo de Test</label>
          <select
            value={testMode.toString()}
            onChange={(e) => setTestMode(e.target.value === 'true')}
            className="w-full border rounded p-2"
          >
            <option value="false">Deshabilitado</option>
            <option value="true">Habilitado</option>
          </select>
        </div>
        
        {testMode && (
          <div>
            <label className="text-sm font-medium">API Key de Test</label>
            <input
              type="password"
              value={testKey}
              onChange={(e) => setTestKey(e.target.value)}
              placeholder="sk-..."
              className="w-full border rounded p-2"
            />
            <p className="text-xs text-muted-foreground mt-1">
              Esta API key se usar√° para validaciones de prueba y NO se guardar√° en la base de datos.
            </p>
          </div>
        )}
      </div>
    </div>
  );
}
```

2. **Integrar con `page.tsx`**
```typescript
// Solo mostrar si hay una variable de entorno o flag especial
if (process.env.NEXT_PUBLIC_TEST_MODE === 'true') {
  return <TestModePanel />
}
```

**Uso para propietario/desarrollador:**
1. Agregar variable en `.env.local` del frontend:
```bash
NEXT_PUBLIC_TEST_MODE=true
```

2. Reiniciar frontend
3. Usar el panel para habilitar/deshabilitar modo de test
4. Validar API keys usando el modo de test

**Opci√≥n C: Token de Validaci√≥n (Backend + Frontend)**

Si se usa la opci√≥n de tokens:

**Implementaci√≥n en frontend:**

1. **Crear componente de gesti√≥n de tokens**
```typescript
'use client';

import { useState, useEffect } from 'react';
import { fetchEventSource } from '@microsoft/fetch-event-source';

export function TestTokenManager() {
  const [testToken, setTestToken] = useState<string | null>(null);
  const [loading, setLoading] = useState(false);

  // Generar token al montar
  useEffect(() => {
    generateTestToken();
  }, []);

  const generateTestToken = async () => {
    setLoading(true);
    try {
      const res = await fetch(`${API_BASE}/settings/validate-test`);
      const data = await res.json();
      
      if (data.status === 'token_generated') {
        setTestToken(data.test_token);
      }
    } catch (error) {
      console.error('Error generating test token:', error);
    } finally {
      setLoading(false);
    }
  };

  const validateTestKey = async (apiKey: string) => {
    if (!testToken) {
      alert('Primero genera un token de prueba');
      return;
    }
    
    try {
      const res = await fetch(
        `${API_BASE}/settings/validate-test?test_token=${testToken}`,
        {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ provider: 'openai', api_key: apiKey })
        }
      );
      
      const data = await res.json();
      
      if (data.status === 'success') {
        alert(`‚úì API Key v√°lida: ${data.test_response}`);
      } else {
        alert(`‚úó API Key inv√°lida: ${data.message}`);
      }
    } catch (error) {
      console.error('Error validating test key:', error);
    }
  };

  return (
    <div className="p-4 border rounded-lg bg-blue-50">
      <h3 className="font-semibold mb-2">üß™ Gestor de Tokens de Prueba</h3>
      
      <div className="space-y-4">
        <div>
          <p className="text-sm text-muted-foreground mb-2">
            Token actual:
          </p>
          <code className="block p-2 bg-background rounded text-xs">
            {testToken || 'No generado'}
          </code>
        </div>
        
        <button
          onClick={generateTestToken}
          disabled={loading}
          className="px-4 py-2 bg-primary text-primary-foreground rounded"
        >
          {loading ? 'Generando...' : 'Generar Nuevo Token'}
        </button>
      </div>
      
      <div>
        <label className="text-sm font-medium">API Key de Test</label>
        <input
          type="password"
          id="test-api-key"
          placeholder="sk-..."
          className="w-full border rounded p-2"
        />
      </div>
      
      <button
        onClick={() => {
          const apiKeyInput = document.getElementById('test-api-key') as HTMLInputElement;
          validateTestKey(apiKeyInput.value);
        }}
        className="w-full px-4 py-2 bg-green-600 text-white rounded"
      >
        Validar API Key
      </button>
    </div>
  );
}
```

**‚ùì Preguntas Clave (**

1. ¬øDeseas implementar Opci√≥n A (variable de entorno), Opci√≥n B (par√°metro de URL), Opci√≥n C (tokens), o una combinaci√≥n?
2. Si implementamos m√∫ltiples opciones, ¬ødeseas que el frontend soporte cambiar entre ellas f√°cilmente?
3. ¬øDeseas que el modo de test est√© siempre visible en el frontend (para desarrollador) o solo con una variable especial?

---

### Tarea 9.4: Testing y Validaci√≥n

**Objetivo:** Probar completamente la funcionalidad de validaci√≥n de API key de test.

**Casos de Prueba:**

#### 9.4.1: Pruebas de Backend

1. **Validaci√≥n exitosa**
   - Enviar API key v√°lida
   - Verificar que retorne status "success"
   - Verificar que NO se guarde en BD
   - Verificar logs de validaci√≥n

2. **Validaci√≥n fallida - API Key inv√°lida**
   - Enviar API key inv√°lida
   - Verificar que retorne error 401
   - Verificar mensaje de error claro
   - Verificar que NO se guarde en BD

3. **Validaci√≥n fallida - Rate limit**
   - Enviar m√∫ltiples validaciones r√°pidamente (m√°s del l√≠mite)
   - Verificar que retorne error 429
   - Verificar mensaje de rate limit
   - Esperar a que expire la ventana de tiempo
   - Verificar que permita nuevamente

4. **Validaci√≥n fallida - Proveedor no soportado**
   - Enviar proveedor inv√°lido
   - Verificar que retorne error 400
   - Verificar lista de proveedores soportados

5. **Validaci√≥n con modo de test deshabilitado**
   - Llamar al endpoint sin variable de entorno activada
   - Verificar que retorne error 403
   - Verificar mensaje de error espec√≠fico

#### 9.4.2: Pruebas de Seguridad

1. **Exposici√≥n de API key de test**
   - Verificar que la API key NO aparezca en logs
   - Verificar que NO se guarde en ninguna parte
   - Verificar que NO se retorne en ninguna respuesta

2. **Rate limiting**
   - Verificar que previene m√°s de 10 validaciones por hora
   - Verificar que el l√≠mite se resetea despu√©s del tiempo

3. **Acceso no autorizado**
   - Intentar acceder desde IP diferente (simulado)
   - Verificar que el rate limiting funcione por IP

#### 9.4.3: Pruebas de Integraci√≥n (si hay frontend)

1. **Validaci√≥n desde frontend**
   - Usar componente de test
   - Verificar que se pueda generar/validar API keys
   - Verificar que la API key de test se use correctamente

2. **Validaci√≥n con modo de test**
   - Habilitar modo de test
   - Verificar que el endpoint est√© accesible
   - Verificar que el modo de test se pueda deshabilitar

3. **Validaci√≥n sin modo de test**
   - Verificar que el endpoint NO est√© accesible sin modo de test
   - Verificar que la UI normal no muestre opciones de test

**‚ùì Preguntas Clave:**

1. ¬øDeseas crear un script automatizado de pruebas (con pytest o unittest) o pruebas manuales?
2. ¬øQu√© criterios de √©xito considerar para que esta fase est√© completa?
3. ¬øDeseas que incluyamos pruebas de integraci√≥n que prueben el flujo completo (validaci√≥n + uso en workflow)?
4. ¬øDeseas agregar tests de carga para verificar que el endpoint responda correctamente bajo presi√≥n (m√∫ltiples peticiones simult√°neas)?

---

## üìä Summary de Fase 9

### Archivos a Crear

**Backend:**
1. `backend/app/api/endpoints.py` (actualizar) - Endpoint `/api/settings/validate-test`
2. `backend/app/core/test_token_manager.py` - Gesti√≥n de tokens (si usa Opci√≥n C)

**Frontend (si aplica):**
1. `frontend/src/components/test-mode-panel.tsx` - Panel de modo de test (Opci√≥n B)
2. `frontend/src/components/test-token-manager.tsx` - Gestor de tokens (Opci√≥n C)

### Tareas Totales: 4
1. [ ] 9.1: Crear endpoint de validaci√≥n especial
2. [ ] 9.2: Implementar modo de test para propietario
3. [ ] 9.3: Implementaci√≥n en frontend (si aplica)
4. [ ] 9.4: Testing y validaci√≥n

### Preguntas Clave Totales: 19
Distribuidas en cada tarea para facilitar la implementaci√≥n.

---

## üéØ Criterios de √âxito de Fase 9

Al completar esta fase, el sistema deber√°:

1. ‚úÖ Endpoint de validaci√≥n de test implementado (`/api/settings/validate-test`)
2. ‚úÖ API key de test NO se guarda en base de datos
3. ‚úÖ API key de test NO aparece en UI normal
4. ‚úÖ Solo el propietario puede usar la API key de test
5. ‚úÖ Validaci√≥n real con el servicio (OpenAI, Anthropic, etc.)
6. ‚úÖ Rate limiting implementado (opcional pero recomendado)
7. ‚úÖ Logging de validaciones para auditor√≠a
8. ‚úÖ Modo de test f√°cil de habilitar/deshabilitar
9. ‚úÖ Documentaci√≥n clara para desarrollador/propietario
10. ‚úÖ Testing completo de todas las funcionalidades

---

**Fase 9 - Planificaci√≥n Creada Por:** OpenCode Assistant  
**Fecha:** 16 de febrero de 2026  
**Versi√≥n:** 1.0 - Validaci√≥n de API Key de Test  
**Estado:** ‚úÖ LISTA PARA IMPLEMENTACI√ìN
