# Tarea 2.1: Sistema de M煤ltiples Proveedores

##  PRIORIDAD CRTICA

## Objetivo
Implementar un sistema flexible que soporte m煤ltiples proveedores de LLM (OpenAI, Anthropic, Google Gemini, etc.) con una abstracci贸n unificada que permita cambiar entre proveedores sin modificar el c贸digo del workflow.

## Estado Actual
- Sistema hardcoded para un solo proveedor (probablemente OpenAI)
- No hay abstracci贸n de provider
- Modelos espec铆ficos est谩n hardcoded
- No hay detecci贸n din谩mica de modelos disponibles

## Arquitectura Propuesta

### Patr贸n: Strategy Pattern + Factory
```
ProviderManager (Factory)
     OpenAIProvider (Strategy)
     AnthropicProvider (Strategy)
     GoogleProvider (Strategy)
     BaseProvider (Interface)
```

## Pasos de Implementaci贸n

### Paso 1: Crear Abstracci贸n Base de Provider

#### Acci贸n 1.1: Crear BaseProvider Abstract Class

**Crear**: `backend/app/services/providers/base.py`

```python
from abc import ABC, abstractmethod
from typing import Dict, List, Any, AsyncIterator
from pydantic import BaseModel

class ModelInfo(BaseModel):
    """Informaci贸n de un modelo disponible"""
    id: str
    name: str
    description: str
    max_tokens: int
    supports_streaming: bool
    cost_per_1k_tokens: float

class ProviderConfig(BaseModel):
    """Configuraci贸n de un provider"""
    api_key: str
    model: str
    temperature: float = 0.7
    max_tokens: int = 2000

class BaseProvider(ABC):
    """Clase base abstracta para todos los providers de LLM"""
    
    def __init__(self, config: ProviderConfig):
        self.config = config
        self._validate_config()
    
    @abstractmethod
    def _validate_config(self) -> None:
        """Validar configuraci贸n del provider"""
        pass
    
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> str:
        """Generar respuesta de texto"""
        pass
    
    @abstractmethod
    async def generate_stream(self, prompt: str, **kwargs) -> AsyncIterator[str]:
        """Generar respuesta con streaming"""
        pass
    
    @abstractmethod
    def get_available_models(self) -> List[ModelInfo]:
        """Obtener lista de modelos disponibles"""
        pass
    
    @abstractmethod
    async def validate_api_key(self) -> bool:
        """Validar que API key es v谩lida"""
        pass
    
    @property
    @abstractmethod
    def provider_name(self) -> str:
        """Nombre del provider"""
        pass
```

**Crear directorio**:
```bash
mkdir -p backend/app/services/providers
touch backend/app/services/providers/__init__.py
```

#### Acci贸n 1.2: Implementar OpenAIProvider

**Crear**: `backend/app/services/providers/openai_provider.py`

```python
import openai
from typing import List, AsyncIterator
import logging

from .base import BaseProvider, ModelInfo, ProviderConfig

logger = logging.getLogger(__name__)

class OpenAIProvider(BaseProvider):
    """Implementaci贸n de provider para OpenAI"""
    
    MODELS = {
        "gpt-4-turbo": ModelInfo(
            id="gpt-4-turbo-preview",
            name="GPT-4 Turbo",
            description="Modelo m谩s potente y reciente de OpenAI",
            max_tokens=128000,
            supports_streaming=True,
            cost_per_1k_tokens=0.03
        ),
        "gpt-4": ModelInfo(
            id="gpt-4",
            name="GPT-4",
            description="Modelo GPT-4 est谩ndar",
            max_tokens=8192,
            supports_streaming=True,
            cost_per_1k_tokens=0.03
        ),
        "gpt-3.5-turbo": ModelInfo(
            id="gpt-3.5-turbo",
            name="GPT-3.5 Turbo",
            description="Modelo r谩pido y econ贸mico",
            max_tokens=16385,
            supports_streaming=True,
            cost_per_1k_tokens=0.001
        )
    }
    
    def __init__(self, config: ProviderConfig):
        super().__init__(config)
        self.client = openai.AsyncOpenAI(api_key=config.api_key)
    
    def _validate_config(self) -> None:
        if not self.config.api_key.startswith('sk-'):
            raise ValueError("Invalid OpenAI API key format")
        if self.config.model not in self.MODELS:
            raise ValueError(f"Model {self.config.model} not supported")
    
    async def generate(self, prompt: str, **kwargs) -> str:
        """Generar respuesta sin streaming"""
        try:
            response = await self.client.chat.completions.create(
                model=self.config.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                **kwargs
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Error generating with OpenAI: {e}")
            raise
    
    async def generate_stream(self, prompt: str, **kwargs) -> AsyncIterator[str]:
        """Generar respuesta con streaming"""
        try:
            stream = await self.client.chat.completions.create(
                model=self.config.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                stream=True,
                **kwargs
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            logger.error(f"Error streaming with OpenAI: {e}")
            raise
    
    def get_available_models(self) -> List[ModelInfo]:
        """Obtener lista de modelos disponibles"""
        return list(self.MODELS.values())
    
    async def validate_api_key(self) -> bool:
        """Validar API key haciendo una llamada m铆nima"""
        try:
            # Hacer llamada m铆nima para validar
            await self.client.models.list()
            return True
        except openai.AuthenticationError:
            return False
        except Exception as e:
            logger.error(f"Error validating OpenAI key: {e}")
            return False
    
    @property
    def provider_name(self) -> str:
        return "OpenAI"
```

#### Acci贸n 1.3: Implementar AnthropicProvider

**Crear**: `backend/app/services/providers/anthropic_provider.py`

```python
import anthropic
from typing import List, AsyncIterator
import logging

from .base import BaseProvider, ModelInfo, ProviderConfig

logger = logging.getLogger(__name__)

class AnthropicProvider(BaseProvider):
    """Implementaci贸n de provider para Anthropic Claude"""
    
    MODELS = {
        "claude-3-opus": ModelInfo(
            id="claude-3-opus-20240229",
            name="Claude 3 Opus",
            description="Modelo m谩s potente de Claude",
            max_tokens=200000,
            supports_streaming=True,
            cost_per_1k_tokens=0.015
        ),
        "claude-3-sonnet": ModelInfo(
            id="claude-3-sonnet-20240229",
            name="Claude 3 Sonnet",
            description="Balance entre capacidad y velocidad",
            max_tokens=200000,
            supports_streaming=True,
            cost_per_1k_tokens=0.003
        ),
        "claude-3-haiku": ModelInfo(
            id="claude-3-haiku-20240307",
            name="Claude 3 Haiku",
            description="Modelo r谩pido y econ贸mico",
            max_tokens=200000,
            supports_streaming=True,
            cost_per_1k_tokens=0.00025
        )
    }
    
    def __init__(self, config: ProviderConfig):
        super().__init__(config)
        self.client = anthropic.AsyncAnthropic(api_key=config.api_key)
    
    def _validate_config(self) -> None:
        if not self.config.api_key.startswith('sk-ant-'):
            raise ValueError("Invalid Anthropic API key format")
        if self.config.model not in self.MODELS:
            raise ValueError(f"Model {self.config.model} not supported")
    
    async def generate(self, prompt: str, **kwargs) -> str:
        """Generar respuesta sin streaming"""
        try:
            message = await self.client.messages.create(
                model=self.config.model,
                max_tokens=self.config.max_tokens,
                temperature=self.config.temperature,
                messages=[{"role": "user", "content": prompt}],
                **kwargs
            )
            return message.content[0].text
        except Exception as e:
            logger.error(f"Error generating with Anthropic: {e}")
            raise
    
    async def generate_stream(self, prompt: str, **kwargs) -> AsyncIterator[str]:
        """Generar respuesta con streaming"""
        try:
            async with self.client.messages.stream(
                model=self.config.model,
                max_tokens=self.config.max_tokens,
                temperature=self.config.temperature,
                messages=[{"role": "user", "content": prompt}],
                **kwargs
            ) as stream:
                async for text in stream.text_stream:
                    yield text
                    
        except Exception as e:
            logger.error(f"Error streaming with Anthropic: {e}")
            raise
    
    def get_available_models(self) -> List[ModelInfo]:
        return list(self.MODELS.values())
    
    async def validate_api_key(self) -> bool:
        """Validar API key"""
        try:
            # Hacer llamada m铆nima
            await self.client.messages.create(
                model=self.MODELS["claude-3-haiku"].id,
                max_tokens=1,
                messages=[{"role": "user", "content": "test"}]
            )
            return True
        except anthropic.AuthenticationError:
            return False
        except Exception as e:
            logger.error(f"Error validating Anthropic key: {e}")
            return False
    
    @property
    def provider_name(self) -> str:
        return "Anthropic"
```

### Paso 2: Crear ProviderManager (Factory)

**Crear**: `backend/app/services/providers/manager.py`

```python
from typing import Dict, Type, Optional
import logging

from .base import BaseProvider, ProviderConfig
from .openai_provider import OpenAIProvider
from .anthropic_provider import AnthropicProvider

logger = logging.getLogger(__name__)

class ProviderManager:
    """Factory para crear y gestionar providers"""
    
    _providers: Dict[str, Type[BaseProvider]] = {
        "openai": OpenAIProvider,
        "anthropic": AnthropicProvider,
        # Agregar m谩s providers aqu铆
    }
    
    @classmethod
    def get_provider(cls, provider_name: str, config: ProviderConfig) -> BaseProvider:
        """Obtener instancia de provider"""
        provider_class = cls._providers.get(provider_name.lower())
        
        if not provider_class:
            raise ValueError(f"Provider '{provider_name}' not supported")
        
        try:
            return provider_class(config)
        except Exception as e:
            logger.error(f"Error creating provider {provider_name}: {e}")
            raise
    
    @classmethod
    def get_available_providers(cls) -> Dict[str, str]:
        """Obtener lista de providers disponibles"""
        return {
            name: provider_class(
                ProviderConfig(api_key="dummy", model="dummy")
            ).provider_name
            for name, provider_class in cls._providers.items()
        }
    
    @classmethod
    def register_provider(cls, name: str, provider_class: Type[BaseProvider]):
        """Registrar nuevo provider (para extensibilidad)"""
        cls._providers[name.lower()] = provider_class
        logger.info(f"Provider '{name}' registered")
```

### Paso 3: Integrar con LangGraph Workflow

**Modificar**: `backend/app/agents/nodes.py`

```python
from app.services.providers.manager import ProviderManager
from app.services.providers.base import ProviderConfig

async def generate_node(state: GraphState) -> dict:
    """Nodo que genera contenido usando provider configurado"""
    
    # Obtener configuraci贸n desde state o settings
    provider_name = state.get("provider", "openai")
    api_key = state.get("api_key")
    model = state.get("model", "gpt-4")
    
    # Crear config
    config = ProviderConfig(
        api_key=api_key,
        model=model,
        temperature=0.7,
        max_tokens=2000
    )
    
    # Obtener provider
    provider = ProviderManager.get_provider(provider_name, config)
    
    # Generar respuesta
    prompt = state.get("final_prompt", "")
    
    try:
        response = await provider.generate(prompt)
        state["final_output"] = response
        state["status"] = "completed"
    except Exception as e:
        logger.error(f"Error in generate_node: {e}")
        state["error"] = str(e)
        state["status"] = "failed"
    
    return state
```

### Paso 4: Crear Endpoints de Backend

**Crear**: `backend/app/api/providers.py`

```python
from fastapi import APIRouter, HTTPException, Depends
from sqlalchemy.orm import Session
from typing import List

from app.db.database import get_db
from app.services.providers.manager import ProviderManager
from app.services.providers.base import ProviderConfig, ModelInfo

router = APIRouter(prefix="/providers", tags=["providers"])

@router.get("/available")
async def get_available_providers():
    """Obtener lista de providers disponibles"""
    try:
        providers = ProviderManager.get_available_providers()
        return {"providers": providers}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/{provider_name}/models")
async def get_provider_models(provider_name: str):
    """Obtener modelos disponibles para un provider"""
    try:
        # Crear provider con config dummy solo para obtener modelos
        config = ProviderConfig(api_key="dummy", model="dummy")
        provider = ProviderManager.get_provider(provider_name, config)
        models = provider.get_available_models()
        
        return {"models": [model.dict() for model in models]}
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/{provider_name}/validate")
async def validate_provider_key(provider_name: str, api_key: str):
    """Validar API key de un provider"""
    try:
        # Obtener modelo por defecto del provider
        config_dummy = ProviderConfig(api_key="dummy", model="dummy")
        provider_dummy = ProviderManager.get_provider(provider_name, config_dummy)
        models = provider_dummy.get_available_models()
        
        if not models:
            raise HTTPException(status_code=400, detail="No models available")
        
        # Crear provider con API key real
        config = ProviderConfig(
            api_key=api_key,
            model=models[0].id
        )
        provider = ProviderManager.get_provider(provider_name, config)
        
        # Validar
        is_valid = await provider.validate_api_key()
        
        return {"valid": is_valid}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

**Integrar en main.py**:
```python
from app.api import providers

app.include_router(providers.router, prefix="/api")
```

### Paso 5: Frontend - Provider Selector

**Crear**: `frontend/src/components/settings/ProviderSelector.tsx`

```typescript
'use client';

import { useState, useEffect } from 'react';
import { useSettingsStore } from '@/store/settingsStore';

interface Provider {
  name: string;
  displayName: string;
}

export default function ProviderSelector() {
  const [providers, setProviders] = useState<Provider[]>([]);
  const [loading, setLoading] = useState(true);
  const { provider, setProvider } = useSettingsStore();

  useEffect(() => {
    fetchProviders();
  }, []);

  const fetchProviders = async () => {
    try {
      const response = await fetch('/api/providers/available');
      const data = await response.json();
      
      const providerList = Object.entries(data.providers).map(([key, value]) => ({
        name: key,
        displayName: value as string
      }));
      
      setProviders(providerList);
    } catch (error) {
      console.error('Error fetching providers:', error);
    } finally {
      setLoading(false);
    }
  };

  if (loading) {
    return <div>Cargando providers...</div>;
  }

  return (
    <div className="space-y-2">
      <label className="block text-sm font-medium text-gray-700">
        Proveedor de LLM
      </label>
      
      <select
        value={provider}
        onChange={(e) => setProvider(e.target.value)}
        className="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-blue-500"
      >
        <option value="">Seleccionar proveedor</option>
        {providers.map((p) => (
          <option key={p.name} value={p.name}>
            {p.displayName}
          </option>
        ))}
      </select>
    </div>
  );
}
```

### Paso 6: Frontend - Model Selector

**Crear**: `frontend/src/components/settings/ModelSelector.tsx`

```typescript
'use client';

import { useState, useEffect } from 'react';
import { useSettingsStore } from '@/store/settingsStore';

interface Model {
  id: string;
  name: string;
  description: string;
  max_tokens: number;
  cost_per_1k_tokens: number;
}

export default function ModelSelector() {
  const [models, setModels] = useState<Model[]>([]);
  const [loading, setLoading] = useState(false);
  const { provider, model, setModel } = useSettingsStore();

  useEffect(() => {
    if (provider) {
      fetchModels();
    }
  }, [provider]);

  const fetchModels = async () => {
    if (!provider) return;
    
    setLoading(true);
    try {
      const response = await fetch(`/api/providers/${provider}/models`);
      const data = await response.json();
      setModels(data.models);
      
      // Si no hay modelo seleccionado, seleccionar el primero
      if (!model && data.models.length > 0) {
        setModel(data.models[0].id);
      }
    } catch (error) {
      console.error('Error fetching models:', error);
    } finally {
      setLoading(false);
    }
  };

  if (!provider) {
    return (
      <div className="text-sm text-gray-500">
        Selecciona un proveedor primero
      </div>
    );
  }

  if (loading) {
    return <div>Cargando modelos...</div>;
  }

  return (
    <div className="space-y-2">
      <label className="block text-sm font-medium text-gray-700">
        Modelo
      </label>
      
      <select
        value={model}
        onChange={(e) => setModel(e.target.value)}
        className="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-blue-500"
      >
        <option value="">Seleccionar modelo</option>
        {models.map((m) => (
          <option key={m.id} value={m.id}>
            {m.name} - ${m.cost_per_1k_tokens}/1K tokens
          </option>
        ))}
      </select>
      
      {/* Mostrar info del modelo seleccionado */}
      {model && models.find(m => m.id === model) && (
        <div className="mt-2 p-3 bg-gray-50 rounded text-sm">
          <p className="text-gray-700">
            {models.find(m => m.id === model)?.description}
          </p>
          <p className="text-gray-500 mt-1">
            Max tokens: {models.find(m => m.id === model)?.max_tokens.toLocaleString()}
          </p>
        </div>
      )}
    </div>
  );
}
```

## Consideraciones Importantes

### Extensibilidad
- F谩cil agregar nuevos providers (Google, Cohere, etc.)
- Solo implementar BaseProvider interface

### Performance
- Cachear lista de modelos disponibles
- No validar API key en cada request

### Seguridad
- NUNCA loguear API keys completas
- Validar formato antes de hacer llamadas externas

## Preguntas Clave

1. **驴Soportar providers custom del usuario?**
   - Permitir que usuario agregue su propio endpoint compatible

2. **驴Cachear respuestas de validaci贸n?**
   - Evitar validar mismo API key m煤ltiples veces

3. **驴Rate limiting por provider?**
   - Diferentes providers tienen diferentes l铆mites

## Criterios de xito

- [ ] Al menos 2 providers funcionando (OpenAI + Anthropic)
- [ ] Usuario puede cambiar provider desde UI
- [ ] Modelos se cargan din谩micamente seg煤n provider
- [ ] Validaci贸n de API key funciona
- [ ] No hay breaking changes en workflow existente

## Notas Importantes

锔 **IMPORTANTE**: Al completar, actualizar `PROGRESS.md` marcando tarea 2.1 como completada.

 **DEPENDENCIAS**: Instalar `anthropic` en backend:
```bash
poetry add anthropic
```

 **BACKWARD COMPATIBILITY**: Mantener funcionamiento con configuraci贸n anterior mientras se migra.
